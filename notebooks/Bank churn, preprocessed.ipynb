{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-processed data from Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create our Sagemaker session and role, and create a S3 prefix to use for the notebook example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()\n",
    "\n",
    "# S3 prefix\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'dbt-preprocessed-churn'\n",
    "WORK_DIRECTORY = 'dbt-preprocessed-churn'\n",
    "# Snowflake credentials\n",
    "ssm_client = sagemaker_session.boto_session.client(service_name='ssm',region_name='ap-southeast-2')\n",
    "snowflake_account = ssm_client.get_parameter(Name='snowflake_account',WithDecryption=False)['Parameter']['Value']\n",
    "snowflake_user = ssm_client.get_parameter(Name='snowflake_user',WithDecryption=False)['Parameter']['Value']\n",
    "snowflake_password = ssm_client.get_parameter(Name='snowflake_password',WithDecryption=True)['Parameter']['Value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data and training the model <a class=\"anchor\" id=\"training\"></a>\n",
    "## Downloading dataset <a class=\"anchor\" id=\"download_data\"></a>\n",
    "SageMaker team has downloaded the dataset from UCI and uploaded to one of the S3 buckets in our account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try this with raw Snowflake input\n",
    "import snowflake.connector\n",
    "ctx = snowflake.connector.connect(\n",
    "  user=snowflake_user,\n",
    "  password=snowflake_password,\n",
    "  account=snowflake_account\n",
    ")\n",
    "cs=ctx.cursor()\n",
    "allrows=cs.execute( \\\n",
    "\"select EXITED,CREDITSCORE_SCALED,AGE_SCALED,TENURE_SCALED,BALANCE_SCALED,NUMOFPRODUCTS_SCALED,ESTIMATEDSALARY_SCALED, \\\n",
    "HASCRCARD,ISACTIVEMEMBER,GEOG_FRANCE,GEOG_SPAIN,GEOG_GERMANY,GENDER_FEMALE,GENDER_MALE from \\\"DEMO_DB\\\".\\\"DBT_ML_PIPELINE\\\".\\\"CHURN_PREPROCESSED\\\"\").fetchall()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = np.array(allrows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into test/train and write to S3\n",
    "To meet the expectations of the Keras script, we'll the preprocessed csv into the four test+train features+labels files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y=dataset[:,0]\n",
    "x=dataset[:,1:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "split_files_path=f'./{WORK_DIRECTORY}/preprocessed/split'\n",
    "from pathlib import Path\n",
    "Path(split_files_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.save(f'{split_files_path}/train_X.npy', X_train)\n",
    "np.save(f'{split_files_path}/train_Y.npy', y_train)\n",
    "np.save(f'{split_files_path}/test_X.npy', X_test)\n",
    "np.save(f'{split_files_path}/test_Y.npy', y_test)\n",
    "\n",
    "data_dir = sagemaker_session.upload_data(path=f'{split_files_path}', bucket=bucket, key_prefix='preprocessed_split_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train as a SageMaker training job\n",
    "\n",
    "The TensorFlow estimator uses the `keras_ann.py` script as the entrypoint. Give special attention to the `keras_model_fn` which was re-defined within this python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based-off: https://github.com/aws-samples/amazon-sagemaker-script-mode/blob/master/keras-embeddings-script-mode/keras-embeddings.ipynb\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "s3_tf_output_key_prefix = \"tf_training_output\"\n",
    "s3_tf_output_location = 's3://{}/{}/{}/{}'.format(bucket, prefix, s3_tf_output_key_prefix, 'tf_model')\n",
    "\n",
    "tf_estimator_sm = TensorFlow(\n",
    "    entry_point=\"keras_ann_script_mode.py\",\n",
    "    role=role,\n",
    "    model_dir=s3_tf_output_location,\n",
    "    framework_version=\"1.12.0\",\n",
    "    train_instance_count=1, \n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    hyperparameters={'learning_rate': 0.1, \n",
    "                     'epochs': 1, \n",
    "                     'batch_size': 10},\n",
    "    script_mode=True,\n",
    "    py_version=\"py3\"\n",
    ")\n",
    "\n",
    "tf_estimator_sm.fit({'train': data_dir, 'eval': data_dir})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the ANN to SageMaker and expose as an endpoint\n",
    "\n",
    "If we wish to deploy the model to production, the next step is to create a SageMaker hosted endpoint. The endpoint will retrieve the TensorFlow SavedModel created during training and deploy it within a TensorFlow Serving container. This all can be accomplished with one line of code, an invocation of the Estimator's deploy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_sm = tf_estimator_sm.deploy(instance_type='ml.t2.medium', initial_instance_count=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a request to our pipeline endpoint <a class=\"anchor\" id=\"pipeline_inference_request\"></a>\n",
    "\n",
    "Here we just grab the first line from the test data (you'll notice that the inference python script is very particular about the ordering of the inference request data). The ```ContentType``` field configures the first container, while the ```Accept``` field configures the last container. You can also specify each container's ```Accept``` and ```ContentType``` values using environment variables.\n",
    "\n",
    "We make our request with the payload in ```'text/csv'``` format, since that is what our script currently supports. If other formats need to be supported, this would have to be added to the ```output_fn()``` method in our entry point. Note that we set the ```Accept``` to ```application/json```, since Linear Learner does not support ```text/csv``` ```Accept```. The prediction output in this case is trying to guess the number of rings the abalone specimen would have given its other physical features; the actual number of rings is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = [-0.326221422,0.2935174226,-1.041759689,0.0003237994151,-0.9115834401,0.021886494,1,1,1,0,0,1,0]\n",
    "\n",
    "print(predictor_sm.predict(payload))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Endpoint <a class=\"anchor\" id=\"delete_endpoint\"></a>\n",
    "Once we are finished with the endpoint, we clean up the resources!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = sagemaker_session.boto_session.client('sagemaker')\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
